{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3209332,"sourceType":"datasetVersion","datasetId":1946896},{"sourceId":7753994,"sourceType":"datasetVersion","datasetId":4533880}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MVTec Data Loader Example","metadata":{}},{"cell_type":"markdown","source":"## This notebook provides an example on how to use the MVTec data loader to train a binary classifier\n## This is a modified version of original https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html","metadata":{}},{"cell_type":"markdown","source":"## Import packages","metadata":{}},{"cell_type":"markdown","source":"# IMAGENET-30","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import ImageFilter, Image, ImageOps\nfrom torchvision.datasets.folder import default_loader\nimport os\n\nclass IMAGENET30_TEST_DATASET(Dataset):\n    def __init__(self, root_dir=\"/kaggle/input/imagenet30-dataset/one_class_test/one_class_test/\", transform=None):\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the classes.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.img_path_list = []\n        self.targets = []\n\n        # Map each class to an index\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(sorted(os.listdir(root_dir)))}\n        print(f\"self.class_to_idx in ImageNet30_Test_Dataset:\\n{self.class_to_idx}\")\n\n        # Walk through the directory and collect information about the images and their labels\n        for i, class_name in enumerate(os.listdir(root_dir)):\n            class_path = os.path.join(root_dir, class_name)\n            for instance_folder in os.listdir(class_path):\n                instance_path = os.path.join(class_path, instance_folder)\n                if instance_path != \"/kaggle/input/imagenet30-dataset/one_class_test/one_class_test/airliner/._1.JPEG\":\n                    for img_name in os.listdir(instance_path):\n                        if img_name.endswith('.JPEG'):\n                            img_path = os.path.join(instance_path, img_name)\n                            # image = Image.open(img_path).convert('RGB')\n                            self.img_path_list.append(img_path)\n                            self.targets.append(self.class_to_idx[class_name])\n\n    def __len__(self):\n        return len(self.img_path_list)\n\n    def __getitem__(self, idx):\n        img_path = self.img_path_list[idx]\n        image = default_loader(img_path)\n        label = self.targets[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \nimagenet30_testset = IMAGENET30_TEST_DATASET()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:46:12.113039Z","iopub.execute_input":"2024-03-11T11:46:12.113802Z","iopub.status.idle":"2024-03-11T11:46:12.510743Z","shell.execute_reply.started":"2024-03-11T11:46:12.113757Z","shell.execute_reply":"2024-03-11T11:46:12.509914Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"self.class_to_idx in ImageNet30_Test_Dataset:\n{'acorn': 0, 'airliner': 1, 'ambulance': 2, 'american_alligator': 3, 'banjo': 4, 'barn': 5, 'bikini': 6, 'digital_clock': 7, 'dragonfly': 8, 'dumbbell': 9, 'forklift': 10, 'goblet': 11, 'grand_piano': 12, 'hotdog': 13, 'hourglass': 14, 'manhole_cover': 15, 'mosque': 16, 'nail': 17, 'parking_meter': 18, 'pillow': 19, 'revolver': 20, 'rotary_dial_telephone': 21, 'schooner': 22, 'snowmobile': 23, 'soccer_ball': 24, 'stingray': 25, 'strawberry': 26, 'tank': 27, 'toaster': 28, 'volcano': 29}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# mvtecDataset","metadata":{}},{"cell_type":"code","source":"# This is a modified version of original  https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n# This file and the mvtec data directory must be in the same directory, such that:\n# /.../this_directory/mvtecDataset.py\n# /.../this_directory/mvtec/bottle/...\n# /.../this_directory/mvtec/cable/...\n# and so on\n\nfrom __future__ import print_function\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\nimport torch.utils.data as data\nimport matplotlib.image as mpimg\nfrom torchvision import transforms\nimport random\n\n\nfrom PIL import Image\n\ndef center_paste(large_img, small_img):\n    # Calculate the center position\n    large_width, large_height = large_img.size\n    small_width, small_height = small_img.size\n    \n    # Calculate the top-left position\n    left = (large_width - small_width) // 2\n    top = (large_height - small_height) // 2\n    \n    # Create a copy of the large image to keep the original unchanged\n    result_img = large_img.copy()\n    \n    # Paste the small image onto the large one at the calculated position\n    result_img.paste(small_img, (left, top))\n    \n    return result_img\n\nclass MVTEC(data.Dataset):\n    \"\"\"`MVTEC <https://www.mvtec.com/company/research/datasets/mvtec-ad/>`_ Dataset.\n    Args:\n        root (string): Root directory of dataset where directories\n            ``bottle``, ``cable``, etc., exists.\n        train (bool, optional): If True, creates dataset from training set, otherwise\n            creates from test set.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        resize (int, optional): Desired output image size.\n        interpolation (int, optional): Interpolation method for downsizing image.\n        category: bottle, cable, capsule, etc.\n    \"\"\"\n\n\n    def __init__(self, root, train=True,\n                 transform=None, target_transform=None,\n                 category='carpet', resize=None, interpolation=2, use_imagenet=False, select_random_image_from_imagenet=False, shrink_factor=0.9):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train\n        self.resize = resize\n        if use_imagenet:\n            self.resize = int(resize * shrink_factor)\n        self.interpolation = interpolation\n        self.select_random_image_from_imagenet = select_random_image_from_imagenet\n        \n        # load images for training\n        if self.train:\n            self.train_data = []\n            self.train_labels = []\n            cwd = os.getcwd()\n            trainFolder = self.root+'/'+category+'/train/good/'\n            os.chdir(trainFolder)\n            filenames = [f.name for f in os.scandir()]\n            for file in filenames:\n                img = mpimg.imread(file)\n                img = img*255\n                img = img.astype(np.uint8)\n                self.train_data.append(img)\n                self.train_labels.append(1)                 \n            os.chdir(cwd)\n                \n            self.train_data = np.array(self.train_data)      \n        else:\n        # load images for testing\n            self.test_data = []\n            self.test_labels = []\n            \n            cwd = os.getcwd()\n            testFolder = self.root+'/'+category+'/test/'\n            os.chdir(testFolder)\n            subfolders = [sf.name for sf in os.scandir() if sf.is_dir()]\n#             print(subfolders)\n            cwsd = os.getcwd()\n            \n            # for every subfolder in test folder\n            for subfolder in subfolders:\n                label = 0\n                if subfolder == 'good':\n                    label = 1\n                testSubfolder = testFolder+subfolder+'/'\n#                 print(testSubfolder)\n                os.chdir(testSubfolder)\n                filenames = [f.name for f in os.scandir()]\n                for file in filenames:\n                    img = mpimg.imread(file)\n                    img = img*255\n                    img = img.astype(np.uint8)\n                    self.test_data.append(img)\n                    self.test_labels.append(label)\n                os.chdir(cwsd)\n            os.chdir(cwd)\n                \n            self.test_data = np.array(self.test_data)\n                \n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img)\n        \n        if self.select_random_image_from_imagenet:\n            imagenet30_img = imagenet30_testset[int(random.random() * len(imagenet30_testset))][0].resize((224, 224))\n        else:\n            imagenet30_img = imagenet30_testset[100][0].resize((224, 224))\n        \n        \n        #if resizing image\n        if self.resize is not None:\n            resizeTransf = transforms.Resize(self.resize, self.interpolation)\n            img = resizeTransf(img)\n            \n#         print(f\"imagenet30_img.size: {imagenet30_img.size}\")\n#         print(f\"img.size: {img.size}\")\n        img = center_paste(imagenet30_img, img)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        \n        return img, target\n\n    def __len__(self):\n        \"\"\"\n        Args:\n            None\n        Returns:\n            int: length of array.\n        \"\"\"\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T13:09:31.130979Z","iopub.execute_input":"2024-03-11T13:09:31.131324Z","iopub.status.idle":"2024-03-11T13:09:31.155359Z","shell.execute_reply.started":"2024-03-11T13:09:31.131299Z","shell.execute_reply":"2024-03-11T13:09:31.154372Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:13:32.210078Z","iopub.execute_input":"2024-03-11T12:13:32.210447Z","iopub.status.idle":"2024-03-11T12:13:32.214759Z","shell.execute_reply.started":"2024-03-11T12:13:32.210420Z","shell.execute_reply":"2024-03-11T12:13:32.213811Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Data loaders","metadata":{}},{"cell_type":"markdown","source":"##############################################################################################\n### To use our data loader please download all the MVTec data available at:- \n### https://www.mvtec.com/company/research/datasets/mvtec-ad\n### And save them in the folder ./mvtec\n##############################################################################################","metadata":{}},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:30:49.888973Z","iopub.execute_input":"2024-03-11T12:30:49.889654Z","iopub.status.idle":"2024-03-11T12:31:05.634137Z","shell.execute_reply.started":"2024-03-11T12:30:49.889622Z","shell.execute_reply":"2024-03-11T12:31:05.633175Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm import tqdm\nimport faiss\nfrom sklearn.metrics import roc_auc_score\n\ndef knn_score(train_set, test_set, n_neighbours=2):\n    \"\"\"\n    Calculates the KNN distance\n    \"\"\"\n    index = faiss.IndexFlatL2(train_set.shape[1])\n    index.add(train_set)\n    D, _ = index.search(test_set, n_neighbours)\n    return np.sum(D, axis=1)\n\n\ndef get_score_knn_auc(model, device, train_feature_space, test_loader, bd_test_loader=False):\n    model.to(device)\n    model.eval()\n\n    test_feature_space = []\n    test_labels = []\n    with torch.no_grad():\n        for idx, (imgs, labels) in enumerate(test_loader):\n            imgs = imgs.to(device)\n            features = model(imgs)\n            test_feature_space.append(features)\n            test_labels.append(labels)\n        test_feature_space = torch.cat(test_feature_space, dim=0).contiguous().cpu().numpy()\n        test_labels = torch.cat(test_labels, dim=0).cpu().numpy()\n\n    distances = knn_score(train_feature_space, test_feature_space)\n\n    auc = roc_auc_score(test_labels, -1 * distances) # I multiplied distances(scores) by -1 because here in dist label is 1\n\n#     print(f\"knn_auc: {auc}\")\n\n    return auc\n\n\ndef eval_step_knn_auc(\n        device,\n        model,\n        train_loader,\n        test_dataloader_ood\n):\n    model.to(device)\n    model.eval()\n    train_feature_space = []\n    with torch.no_grad():\n        for idx, (imgs, labels) in enumerate(train_loader, start=1):\n            imgs = imgs.to(device)\n            features = model(imgs)\n            train_feature_space.append(features)\n        train_feature_space = torch.cat(train_feature_space, dim=0).contiguous().cpu().numpy()\n\n    knn_clean_test_auc = get_score_knn_auc(model, device, train_feature_space, test_dataloader_ood, bd_test_loader=False)\n\n    return knn_clean_test_auc","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:31:53.348142Z","iopub.execute_input":"2024-03-11T12:31:53.349151Z","iopub.status.idle":"2024-03-11T12:31:53.361182Z","shell.execute_reply.started":"2024-03-11T12:31:53.349106Z","shell.execute_reply":"2024-03-11T12:31:53.360264Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass Model(torch.nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n        if backbone == 'ViT': # ViT\n            self.backbone = torchvision.models.vit_b_16(weights='DEFAULT')\n        elif backbone == 152:\n            self.backbone = models.resnet152(pretrained=True)\n        else:\n            self.backbone = models.resnet18(pretrained=True)\n        self.backbone.fc = torch.nn.Identity()\n        freeze_parameters(self.backbone, backbone, train_fc=False)\n\n    def forward(self, x):\n        z1 = self.backbone(x)\n        z_n = F.normalize(z1, dim=-1)\n        return z_n\n    \ndef freeze_parameters(model, backbone, train_fc=False):\n    if not train_fc:\n        for p in model.fc.parameters():\n            p.requires_grad = False\n    if backbone == 152:\n        for p in model.conv1.parameters():\n            p.requires_grad = False\n        for p in model.bn1.parameters():\n            p.requires_grad = False\n        for p in model.layer1.parameters():\n            p.requires_grad = False\n        for p in model.layer2.parameters():\n            p.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:31:57.770145Z","iopub.execute_input":"2024-03-11T12:31:57.770467Z","iopub.status.idle":"2024-03-11T12:31:57.780356Z","shell.execute_reply.started":"2024-03-11T12:31:57.770444Z","shell.execute_reply":"2024-03-11T12:31:57.779453Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def list_directories(path):\n    \"\"\"\n    Returns a list of directory names found in the given path.\n    \n    :param path: The path of the directory to list subdirectories from.\n    :return: A list of directory names.\n    \"\"\"\n    # List all entries in the given path\n    entries = os.listdir(path)\n    \n    # Filter out entries that are directories\n    dir_names = [entry for entry in entries if os.path.isdir(os.path.join(path, entry))]\n    \n    return dir_names\n\nall_categories = list_directories(\"/kaggle/input/mvtec-ad\")\nprint(all_categories)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T13:03:51.580184Z","iopub.execute_input":"2024-03-11T13:03:51.580556Z","iopub.status.idle":"2024-03-11T13:03:51.588738Z","shell.execute_reply.started":"2024-03-11T13:03:51.580530Z","shell.execute_reply":"2024-03-11T13:03:51.587918Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"['wood', 'screw', 'metal_nut', 'capsule', 'hazelnut', 'carpet', 'pill', 'grid', 'zipper', 'transistor', 'tile', 'leather', 'toothbrush', 'bottle', 'cable']\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision import models\n\nfor shrink_factor in [0.8, 0.85, 0.9, 0.95, 1]:\n    transform = transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    batch_size = 70\n\n    # Original images are high resolution, so we resize them using the transformation\n    # provided by pytorch: https://pytorch.org/vision/stable/transforms.html\n    im_shape = 224\n\n    # Interpolation method for resizing the image\n    interpol = 3\n\n    # Data category to use: carpet, leather, wood, bottle, etc.\n    auc_dict = {}\n    auc_sum = 0.0\n    \n    for cat in all_categories:\n        trainset = MVTEC(root='/kaggle/input/mvtec-ad/', train=True, transform=transform,\n                            resize=im_shape, interpolation=interpol, category=cat, use_imagenet=True, select_random_image_from_imagenet=True, shrink_factor=shrink_factor)\n\n        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                                  shuffle=True)\n\n        testset = MVTEC(root='/kaggle/input/mvtec-ad/', train=False, transform=transform,\n                            resize=im_shape, interpolation=interpol, category=cat, use_imagenet=True, select_random_image_from_imagenet=True, shrink_factor=shrink_factor)\n\n        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                                 shuffle=False)\n\n        # Each category has two types of images: good (label 1) or defective (label 0)\n        classes = ('defective', 'good')\n\n        device='cuda:0'\n        model = Model('ViT')\n        model.to(device)\n        auc = eval_step_knn_auc(device, model, trainloader, testloader)\n        auc_sum += auc\n        auc_dict[cat] = auc\n        print(f\"({shrink_factor}, {cat}) -> {auc}\")\n    print(f\"{shrink_factor} -> {auc_dict}\")\n    print(f\"auc mean ({shrink_factor}): {auc_sum / len(all_categories)}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-11T13:15:29.206233Z","iopub.execute_input":"2024-03-11T13:15:29.206613Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 140MB/s] \nTest set feature extracting: 2it [00:01,  1.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.6850877192982456\n(0.8, wood) -> 0.6850877192982456\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 3it [00:02,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.47509735601557695\n(0.8, screw) -> 0.47509735601557695\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:01,  1.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.6627565982404692\n(0.8, metal_nut) -> 0.6627565982404692\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:02,  1.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.6071001196649382\n(0.8, capsule) -> 0.6071001196649382\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:01,  1.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.8571428571428571\n(0.8, hazelnut) -> 0.8571428571428571\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:02,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.5642054574638844\n(0.8, carpet) -> 0.5642054574638844\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 3it [00:02,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.5561920349154392\n(0.8, pill) -> 0.5561920349154392\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:01,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.4912280701754386\n(0.8, grid) -> 0.4912280701754386\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 3it [00:01,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.5383403361344538\n(0.8, zipper) -> 0.5383403361344538\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:01,  1.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.71625\n(0.8, transistor) -> 0.71625\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:01,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.5386002886002886\n(0.8, tile) -> 0.5386002886002886\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:02,  1.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.5492527173913043\n(0.8, leather) -> 0.5492527173913043\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 1it [00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.6833333333333333\n(0.8, toothbrush) -> 0.6833333333333333\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 2it [00:01,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.7658730158730159\n(0.8, bottle) -> 0.7658730158730159\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTest set feature extracting: 3it [00:02,  1.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"knn_auc: 0.8135307346326837\n(0.8, cable) -> 0.8135307346326837\n0.8 -> {'wood': 0.6850877192982456, 'screw': 0.47509735601557695, 'metal_nut': 0.6627565982404692, 'capsule': 0.6071001196649382, 'hazelnut': 0.8571428571428571, 'carpet': 0.5642054574638844, 'pill': 0.5561920349154392, 'grid': 0.4912280701754386, 'zipper': 0.5383403361344538, 'transistor': 0.71625, 'tile': 0.5386002886002886, 'leather': 0.5492527173913043, 'toothbrush': 0.6833333333333333, 'bottle': 0.7658730158730159, 'cable': 0.8135307346326837}\nauc mean (0.8): 0.633599375925462\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}